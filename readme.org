Need to:
- generate a bunch of random graphs to measure how much our algorithm reduces search space.
- generate SCMs for those graphs (not sure how exactly yet!) to test intervention selection on them of two types:
  + deterministic and atomic (just grid search and confirm best intervention is in Sup(G)).
  + stochastic and conditional (best arm identification multi-arm bandits, maybe UCB or LUCB algos).
OR: Just use 3 graphs of increasing size/complexity and test my stuff on those? Like [[cite:&lee2018structural]] does. 

UPDATE:
From meeting with Thijs (2024-11-06):
- Generate a bunch of random graphs with (e.g.) 10 nodes, 100 nodes and 1000 nodes, and with two values of edge density, to measure how much our algorithm reduces search space. We thus get 6 categories of graphs (3x2).
  + Maybe also expore how the effectiveness of the algo relates to treewidth and edge density: we expect that the higher the more it looks like a tree the more it will help with search space reduction.
  + Not sure how to generate yet. Maybe just generating upper triangular matrices to create adjacency matrices, but still need to ensure acyclicity and connectedness.
- Select (at random) one graph of each category. Use binary variables everywhere and generate CPDs (in the form of transition matrices) randomly generated for each graph. The result is 6 CBNs.
- Similarly to Lee's paper, run TS and kl-UCB twice on each CBN (for, say, 1000 iterations/trials), once using the mGISS and once using all nodes (brute-force) as search space. Record the cumulative regret and probability of selecting the best arm at each iteration/trial.

UPDATE:
2024-12-10
- Choosing a conditional intervention means:
  1. Choosing a node X to intervene on.
  2. Given the context an(X) of that node, select a value x to set X to.
- Point 2. corresponds to a contextual bandit problem. For a small number of contexts (which I think we can assume if variables are binary and we have small graphs (and thus small cardinality of an(X))), we can, in the stochastic bandits case, use UCB and TS over maps from contexts to values of X (thus: over pairs (an(x), x)).
  In the adversarial bandits case (and also in the stochastic one) we can use EXP3, or EXP4 over a set of policies to explore.
- Choosing the node may be done by sampling from a non-context policy over all nodes we're considering, I guess. So the probability of choosing an arm do(X=x) equals p(node=X)p(value=x|context=c).
